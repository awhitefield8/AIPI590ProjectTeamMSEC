{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwYFhs6oE9CDkZ30YU5fFo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4__ogVu6XJ4","executionInfo":{"status":"ok","timestamp":1671043644360,"user_tz":300,"elapsed":7867,"user":{"displayName":"Alexander Whitefield","userId":"05450624364286253323"}},"outputId":"61fc9648-122d-41e9-e0a7-814b3a5c211f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n","Collecting trfl\n","  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 12.6 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from trfl) (1.3.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from trfl) (1.14.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from trfl) (0.1.7)\n","Installing collected packages: trfl\n","Successfully installed trfl-1.2.0\n"]}],"source":["! pip install pandas trfl"]},{"cell_type":"code","source":["from google.colab import drive, files\n","import os\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAX1b3QY6bO3","executionInfo":{"status":"ok","timestamp":1671044023478,"user_tz":300,"elapsed":14152,"user":{"displayName":"Alexander Whitefield","userId":"05450624364286253323"}},"outputId":"edd92bef-500e-4f68-deb8-7fc07214178b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["proj_path ='/content/gdrive/MyDrive/project-recommendations/src/models/' \n","\n","#os.listdir(proj_path)\n","os.chdir(proj_path)"],"metadata":{"id":"_iJpWKRw6cl6","executionInfo":{"status":"ok","timestamp":1671044031623,"user_tz":300,"elapsed":473,"user":{"displayName":"Alexander Whitefield","userId":"05450624364286253323"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import time\n","start_t = time.time()\n","! python SA2C.py --model=SASRec --epoch=30 --data=../../data/data_hm --results_path=../../results/SA2C_SASREC_HM.csv\n","duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_t))\n","print(f'time consumed {duration}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDJvPnkJ6d5N","executionInfo":{"status":"ok","timestamp":1671056869649,"user_tz":300,"elapsed":4882340,"user":{"displayName":"Alexander Whitefield","userId":"05450624364286253323"}},"outputId":"2c587681-e1b1-4837-d49f-810ceac620e9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["SA2C.py:188: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  self.seq = tf.compat.v1.layers.dropout(self.seq,\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/content/gdrive/MyDrive/project-recommendations/src/models/SASRecModules.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","SA2C.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","SA2C.py:219: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","2022-12-14 21:06:38.309581: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","the loss in 200th batch is: 12.793850\n","the loss in 400th batch is: 11.146348\n","the loss in 600th batch is: 10.605709\n","the loss in 800th batch is: 10.499564\n","the loss in 1000th batch is: 10.377942\n","the loss in 1200th batch is: 10.168329\n","the loss in 1400th batch is: 9.850037\n","the loss in 1600th batch is: 9.736603\n","the loss in 1800th batch is: 9.743774\n","the loss in 2000th batch is: 9.702161\n","the loss in 2200th batch is: 9.550572\n","the loss in 2400th batch is: 9.566997\n","the loss in 2600th batch is: 8.676851\n","the loss in 2800th batch is: 8.918659\n","the loss in 3000th batch is: 8.634941\n","the loss in 3200th batch is: 8.639193\n","the loss in 3400th batch is: 8.399220\n","the loss in 3600th batch is: 7.881660\n","the loss in 3800th batch is: 7.897603\n","the loss in 4000th batch is: 7.973903\n","tcmalloc: large alloc 1851318272 bytes == 0x82eca000 @  0x7f5a98cc81e7 0x7f5a6353014e 0x7f5a63588745 0x7f5a63588878 0x7f5a635ce597 0x7f5a635d27dc 0x7f5a63621a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f5a63572944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n","tcmalloc: large alloc 1614995456 bytes == 0xf5458000 @  0x7f5a98cc81e7 0x7f5a6353014e 0x7f5a63588745 0x7f5a63588878 0x7f5a635ce597 0x7f5a635d27dc 0x7f5a63621a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f5a63572944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n","tcmalloc: large alloc 1684275200 bytes == 0x82eca000 @  0x7f5a98cc81e7 0x7f5a6353014e 0x7f5a63588745 0x7f5a63588878 0x7f5a635ce597 0x7f5a635d27dc 0x7f5a63621a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f5a63572944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n","tcmalloc: large alloc 1691205632 bytes == 0xf5458000 @  0x7f5a98cc81e7 0x7f5a6353014e 0x7f5a63588745 0x7f5a63588878 0x7f5a635ce597 0x7f5a635d27dc 0x7f5a63621a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f5a63572944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n","#############################################################\n","total clicks: 0, total purchase:41673\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 2080.000000\n","clicks hr ndcg @ 5 : 0.000000, 0.000000\n","purchase hr and ndcg @5 : 0.049912, 0.040096\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 2375.000000\n","clicks hr ndcg @ 10 : 0.000000, 0.000000\n","purchase hr and ndcg @10 : 0.056991, 0.042390\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 2550.000000\n","clicks hr ndcg @ 15 : 0.000000, 0.000000\n","purchase hr and ndcg @15 : 0.061191, 0.043500\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 2686.000000\n","clicks hr ndcg @ 20 : 0.000000, 0.000000\n","purchase hr and ndcg @20 : 0.064454, 0.044270\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 1.000000, 0.008455\n","#############################################################\n","the loss in 4200th batch is: 7.676478\n","the loss in 4400th batch is: 7.440216\n","the loss in 4600th batch is: 7.685394\n","the loss in 4800th batch is: 7.154768\n","the loss in 5000th batch is: 7.050427\n","the loss in 5200th batch is: 7.060741\n","the loss in 5400th batch is: 7.072550\n","the loss in 5600th batch is: 6.797444\n","the loss in 5800th batch is: 6.400830\n","the loss in 6000th batch is: 6.529691\n","the loss in 6200th batch is: 6.061151\n","the loss in 6400th batch is: 6.166725\n","the loss in 6600th batch is: 6.072062\n","the loss in 6800th batch is: 5.800888\n","the loss in 7000th batch is: 5.770046\n","the loss in 7200th batch is: 5.612886\n","the loss in 7400th batch is: 5.895290\n","the loss in 7600th batch is: 5.699334\n","the loss in 7800th batch is: 5.595672\n","the loss in 8000th batch is: 5.734228\n","tcmalloc: large alloc 1851318272 bytes == 0x82eca000 @  0x7f5a98cc81e7 0x7f5a6353014e 0x7f5a63588745 0x7f5a63588878 0x7f5a635ce597 0x7f5a635d27dc 0x7f5a63621a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f5a63572944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n","#############################################################\n","total clicks: 0, total purchase:41673\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 2056.000000\n","clicks hr ndcg @ 5 : 0.000000, 0.000000\n","purchase hr and ndcg @5 : 0.049337, 0.036493\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 2488.000000\n","clicks hr ndcg @ 10 : 0.000000, 0.000000\n","purchase hr and ndcg @10 : 0.059703, 0.039871\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 2726.000000\n","clicks hr ndcg @ 15 : 0.000000, 0.000000\n","purchase hr and ndcg @15 : 0.065414, 0.041390\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 2867.000000\n","clicks hr ndcg @ 20 : 0.000000, 0.000000\n","purchase hr and ndcg @20 : 0.068798, 0.042189\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 1.000000, 0.009268\n","#############################################################\n","the loss in 8200th batch is: 5.523582\n","the loss in 8400th batch is: 5.165043\n","the loss in 8600th batch is: 5.124226\n","the loss in 8800th batch is: 5.261297\n","the loss in 9000th batch is: 5.099161\n","the loss in 9200th batch is: 4.936443\n","the loss in 9400th batch is: 4.963675\n","the loss in 9600th batch is: 4.648740\n","the loss in 9800th batch is: 4.926789\n","the loss in 10000th batch is: 4.707474\n","the loss in 10200th batch is: 4.773104\n","the loss in 10400th batch is: 4.853528\n","the loss in 10600th batch is: 4.531346\n","the loss in 10800th batch is: 4.306253\n","the loss in 11000th batch is: 4.402581\n","the loss in 11200th batch is: 4.369953\n","the loss in 11400th batch is: 4.331703\n","the loss in 11600th batch is: 4.174862\n","the loss in 11800th batch is: 4.179627\n","the loss in 12000th batch is: 4.235308\n","#############################################################\n","total clicks: 0, total purchase:41673\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 1588.000000\n","clicks hr ndcg @ 5 : 0.000000, 0.000000\n","purchase hr and ndcg @5 : 0.038106, 0.026396\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 2080.000000\n","clicks hr ndcg @ 10 : 0.000000, 0.000000\n","purchase hr and ndcg @10 : 0.049912, 0.030213\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 2354.000000\n","clicks hr ndcg @ 15 : 0.000000, 0.000000\n","purchase hr and ndcg @15 : 0.056487, 0.031954\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 2532.000000\n","clicks hr ndcg @ 20 : 0.000000, 0.000000\n","purchase hr and ndcg @20 : 0.060759, 0.032963\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 1.000000, 0.008059\n","#############################################################\n","the loss in 12200th batch is: 4.525030\n","the loss in 12400th batch is: 4.087801\n","the loss in 12600th batch is: 4.157705\n","the loss in 12800th batch is: 3.954356\n","the loss in 13000th batch is: 4.111240\n","the loss in 13200th batch is: 4.079889\n","the loss in 13400th batch is: 3.897128\n","the loss in 13600th batch is: 3.815073\n","the loss in 13800th batch is: 3.727437\n","the loss in 14000th batch is: 3.758130\n","the loss in 14200th batch is: 3.704257\n","the loss in 14400th batch is: 3.911271\n","the loss in 14600th batch is: 3.769514\n","the loss in 14800th batch is: 3.513323\n","the loss in 15000th batch is: 3.372314\n","the loss in 15200th batch is: 2.484380\n","the loss in 15400th batch is: 2.731439\n","the loss in 15600th batch is: 2.806640\n","the loss in 15800th batch is: 2.569101\n","the loss in 16000th batch is: 2.620806\n","#############################################################\n","total clicks: 0, total purchase:41673\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 1378.000000\n","clicks hr ndcg @ 5 : 0.000000, 0.000000\n","purchase hr and ndcg @5 : 0.033067, 0.022804\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 1854.000000\n","clicks hr ndcg @ 10 : 0.000000, 0.000000\n","purchase hr and ndcg @10 : 0.044489, 0.026527\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 2118.000000\n","clicks hr ndcg @ 15 : 0.000000, 0.000000\n","purchase hr and ndcg @15 : 0.050824, 0.028202\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 2311.000000\n","clicks hr ndcg @ 20 : 0.000000, 0.000000\n","purchase hr and ndcg @20 : 0.055456, 0.029295\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 1.000000, 0.007019\n","#############################################################\n","the loss in 16200th batch is: 2.638357\n","the loss in 16400th batch is: 2.463380\n","the loss in 16600th batch is: 2.575908\n","the loss in 16800th batch is: 2.672525\n","the loss in 17000th batch is: 2.433693\n","the loss in 17200th batch is: 2.481779\n","the loss in 17400th batch is: 2.568466\n","time consumed 01:21:22\n"]}]}]}